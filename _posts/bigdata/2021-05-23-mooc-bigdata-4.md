---
layout: post
title: 《大数据开发工程师》阶段四：Flink + 高频实时数据处理方案 
category: big-data
tags: [big-data]
---

Flink + 高频实时数据处理方案 

[入门必读！Apache Flink 零基础系列教程](https://developer.aliyun.com/article/753999?utm_content=g_1000138786)

## 阶段四：Flink + 高频实时数据处理方案  
### 第14周   消息队列之Kafka从入门到小牛   
1、什么是消息队列  

2、什么是Kafka  

3、Zookeeper安装部署之单机模式和集群模式  

4、 Kafka安装部署之单机模式和集群模式  

5、Kafka中的生产者和消费者  

6、案例：QQ群聊天  

7、Broker扩展内容  

8、Producer扩展内容  

9、Consumer扩展内容  

10、Topic+Partition+Message扩展内容  

11、Kafka中的存储策略  

12、Kafka中的容错机制  

13、Java代码实现生产者代码  

14、Java代码实现消费者代码  

15、消费者代码扩展  

16、Consumer消费Offset查询  

17、Consumer消费顺序  

18、Kafka的三种语义  

19、Kafka参数调忧之JVM参数调忧  

20、Kafka参数调忧之Replication参数调忧  

21、Kafka参数调忧之Log参数调忧  

22、Kafka Topic命名小技巧  

23、Kafka集群监控管理工具(CMAK)  

24、实战：Flume集成Kafka  

25、实战：Kafka集群平滑升级  

  

### 第15周   内存数据库Redis-极速上手   
1、快速了解Redis  

2、Redis的安装部署  

3、Redis基础命令  

4、Redis多数据库特性  

5、Redis常用数据类型之String  

6、Redis常用数据类型之Hash  

7、Redis常用数据类型之List  

8、Redis常用数据类型之Set  

9、Redis常用数据类型之Sorted Set  

10、案例：存储高一班的学员信息  

11、Java代码操作Redis之单连接  

12、Java代码操作Redis之连接池  

13、提取RedisUtils工具类  

14、Redis高级特性之expire  

15、Redis高级特性之pipeline  

16、Redis高级特性之info  

17、Redis持久化之RDB  

18、Redis持久化之AOF  

19、Redis的安全策略  

20、Redis监控命令-monitor  

21、Redis架构演进过程  

  

### 第16周   实时计算Flink-快速上手篇   
1、快速了解Flink  

2、Flink Streaming程序开发  

3、Flink Batch程序开发  

4、Flink Standalone集群安装部署  

5、Flink ON YARN的第一种方式  

6、Flink ON YARN的第二种方式  

7、向集群中提交Flink任务  

8、Flink核心API介绍  

9、DataStream API之DataSource  

10、DataStream API之Transformation  

11、DataStream API之分区规则介绍  

12、DataStream API之分区规则的使用  

13、DataStream API之DataSink  

14、DataSet API之DataSource  

15、DataSet API之Transformation  

16、DataSet API之DataSink  

17、Table API 和 SQL介绍  

18、创建TableEnvironment对象  

19、TableAPI和SQL的使用  

20、使用DataStream创建表  

21、使用DataSet创建表  

22、将表转换成DataStream  

22、将表转换成DataSet  

  

### 第17周   SparkStreaming + Flink高级进阶之路   
1、Window的概念和类型  

2、TimeWindow的使用  

3、CountWindow的使用  

4、自定义Window的使用  

5、Window中的增量聚合和全量聚合  

6、Flink中的Time  

7、Watermark的分析  

8、开发Watermark代码  

9、通过数据跟踪观察Watermark  

10、Watermark+EventTime处理乱序数据  

11、延迟数据的三种处理方式  

12、在多并行度下的Watermark应用  

13、Watermark案例总结  

14、并行度介绍及四种设置方式  

15、并行度案例分析  

16、KafkaConsumer的使用  

17、KafkaConsumer消费策略设置  

18、KafkaConsumer的容错  

19、KafkaProducer的使用  

20、KafkaProducer的容错  

21、SparkStreaming的WordCount程序开发  

22、SparkStreaming整合Kafka  

  

