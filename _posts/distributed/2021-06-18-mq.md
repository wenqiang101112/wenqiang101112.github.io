---
layout: post
title: 分布式消息队列-Kafka
category: springcloud
tags: [springcloud]
---

分布式消息队列-Kafka

## 参考资料
- [消息队列MQ](https://blog.csdn.net/yuanlong122716/article/details/104488569)  
- [消息中间件（一）MQ详解及四大MQ比较](https://blog.csdn.net/wqc19920906/article/details/82193316/)  
- [MQ消息队列详解、四大MQ的优缺点分析](https://blog.csdn.net/qq_44240587/article/details/104630567)

## 一、JMS & 消息中间件
### 1.1 JMS（JAVA Message Service,java消息服务）:  
是一个消息服务的**标准或者说是规范**，允许应用程序组件基于JavaEE平台创建、发送、接收和读取消息。它使分布式通信耦合度更低，消息服务更加可靠以及异步性。

消息中间件（MOM：Message Orient middleware）  
用途和优点：   
- 1.将数据从一个应用程序传送到另一个应用程序,或者从软件的一个模块传送到另外一个模块； 
- 2.负责建立网络通信的通道，进行数据的可靠传送。 
- 3.保证数据不重发，不丢失 
- 4.能够实现跨平台操作，能够为不同操作系统上的软件集成技工数据传送服务

### 1.2 MQ：Message Queue，消息队列，是一个消息的接受和转发的容器，可用于消息推送。
顾名思义，它就是一个队列，简单来说就是一个应用程序A将数据丢到一个队列中，由另一个应用程序B从队列中拿到这个数据，再去做一些其他的业务操作。我们把应用程序A叫做生产者，应用程序B叫做消费者，它们之间传输的数据称作消息。   
- ActiveMQ: 一款开源的JMS具体实现。由Apache出品的消息中间件，完全支持JMS1.1和J2EE 1.4规范的 JMS Provider实现，非常快速，支持多种语言的客户端和协议。
- RabbitMQ: https://blog.csdn.net/yuanlong122716/category_9754690.html
- RocketMQ:  
- Kafka: 是一个分布式、分区的、多副本的、多订阅者，基于zookeeper协调的分布式日志系统（也可以当做MQ系统），常见可以用于web/nginx日志、访问日志，消息服务等等
- ZeroMQ: 号称最快的消息队列系统，尤其针对大吞吐量的需求场景。

### 1.3 MQ的[使用场景](https://blog.csdn.net/yuanlong122716/article/details/104488569 ) 
- 解耦  
假设系统A通过调用接口推送数据给B、C、D，如果后续系统E也需要被推送、或者B不再需要被推送呢？  
那我们就需要修改系统A的代码，加上给E推送数据的逻辑，去掉给B推送数据的逻辑。显然A系统和其他系统严重耦合.  
在这个场景中，如果使用 MQ，通过发布订阅模型，就可以实现A和其他系统的解耦。A 产生一条数据，发送到 MQ ，哪个系统需要就去订阅消费，如果某个系统不再需要，取消对消息的订阅即可。
- 异步  
将一些无需即时返回且耗时的操作提取出来，进行了异步处理，而这种异步处理的方式大大的节省了服务器的请求响应时间，从而提高了系统的吞吐量。
- 削峰限流  
商城秒杀活动，对于商城系统，可能在0点左右会有个短暂的高峰期，其余时间的并发量也没那么高，假如我们的后台系统直接操作数据库，平时可能没什么问题，但如果突然有很高的的并发量进来，就会因为MySQL并发量过大导致系统瘫痪。  
如果使用MQ，请求会短期积压在MQ中，后台系统从MQ中分批拉取消息，从而保证数据库不会被压垮。等高峰期一过，系统就会将MQ中积压的消息慢慢解决掉。这就是MQ的"削峰限流"作用。

### 1.4 消息传递模式
- 点对点消息传递模式: 生产者发送一条消息到queue，只有一个消费者能收到。  
在点对点消息系统中，消息持久化到一个队列中。此时，将有一个或多个消费者消费队列中的数据。但是一条消息只能被消费一次。当一个消费者消费了队列中的某条数据之后，该条数据则从消息队列中删除。该模式即使有多个消费者同时消费数据，也能保证数据处理的顺序  
![](http://note.youdao.com/yws/public/resource/eff78eaf2b90b8ec82e7a0cf33314ad4/xmlnote/8411103407CE4571A0119AFFF53A56D2/54656)  
- 发布-订阅消息传递模式  
在发布-订阅消息系统中，消息被持久化到一个topic中。与点对点消息系统不同的是，消费者可以订阅一个或多个topic，消费者可以消费该topic中所有的数据，同一条数据可以被多个消费者消费，数据被消费后不会立马删除。在发布-订阅消息系统中，消息的生产者称为发布者，消费者称为订阅者  
![](http://note.youdao.com/yws/public/resource/eff78eaf2b90b8ec82e7a0cf33314ad4/xmlnote/574BBAE6C6E1412E82288BC24F12A549/54658)  

## 二、Kafka介绍
- [Kafka学习之路 （一）](https://www.cnblogs.com/qingyunzong/p/9004509.html)
- [Kafka学习之路 （二）Kafka的架构](https://www.cnblogs.com/qingyunzong/p/9004593.html)
- [秒懂 kafka HA（高可用）](https://blog.csdn.net/yuanlong122716/article/details/105162293)    
- [大白话 kafka 架构原理](https://blog.csdn.net/yuanlong122716/article/details/104825604) 

Kafka是Apache下的一个子项目，是一个**高性能、跨语言、分布式发布、发布-订阅** 消息队列系统  
 
Kafka相对于ActiveMQ是一个非常轻量级的消息系统.  

主要应用场景是：日志收集系统 和 消息系统。

![](https://wdsheng0i.github.io/assets/images/2021/mq/kafka.png)  
上图中一个topic配置了3个partition。Partition1有两个offset：0和1。Partition2有4个offset。Partition3有1个offset。副本的id和副本所在的机器的id恰好相同。

如果一个topic的副本数为3，那么Kafka将在集群中为每个partition创建3个相同的副本。集群中的每个broker存储一个或多个partition。多个producer和consumer可同时生产和消费数据。

### 2.1.Kafka的[优点](https://www.cnblogs.com/qingyunzong/p/9004509.html)  
- 解耦
- 冗余（副本）:消息队列把数据进行持久化直到它们已经被完全处理
- 扩展性
- 灵活性&峰值处理能力: 秒杀场景
- 可恢复性：加入队列中的消息仍然可以在系统恢复后被处理
- 顺序保证：Kafka保证一个Partition内的消息的有序性，主题topic会有多个分区Partition，所以在整个主题的范围内，是无法保证消息的顺序的，单个分区则可以保证
- 缓冲：
- 异步通信：
 

### 2.2.Kafka中的术语解释
#### broker
- Kafka 集群包含一个或多个服务器，服务器节点称为broker  
- broker存储topic的数据。
    - 如果某topic有N个partition，集群有N个broker，那么每个broker存储该topic的一个partition。
    - 如果某topic有N个partition，集群有(N+M)个broker，那么其中有N个broker存储该topic的一个partition，剩下的M个broker不存储该topic的partition数据
    - 如果某topic有N个partition，集群中broker数目少于N个，那么一个broker存储该topic的一个或多个partition。在实际生产环境中，尽量避免这种情况的发生，这种情况容易导致Kafka集群数据不均衡。

#### Topic
- 每条发布到Kafka集群的消息都有一个类别，这个类别被称为Topic
- 物理上不同Topic的消息分开存储，逻辑上一个Topic的消息虽然保存于一个或多个broker上但用户只需指定消息的Topic即可生产或消费数据而不必关心数据存于何处
- topic会有多个分区Partition，所以在整个主题的范围内，是无法保证消息的顺序的

#### Partition
- topic中的数据分割为一个或多个partition。每个topic至少有一个partition
- 每个partition中的数据使用多个segment文件存储
- partition中的数据是有序的，同一topic不同partition间的数据丢失了数据的顺序
- 在需要严格保证消息的消费顺序的场景下，需要将partition数目设为1

#### Producer
- 生产者即数据的发布者，该角色将消息发布到Kafka的topic中
- broker接收到生产者发送的消息后，broker将该消息追加到当前用于追加数据的segment文件中。
- 生产者发送的消息，存储到一个partition中，生产者也可以指定数据存储的partition。

#### Consumer
- 消费者可以从broker中读取数据。
- 消费者可以消费多个topic中的数据

#### Consumer Group
- 每个Consumer属于一个特定的Consumer Group
- 可为每个Consumer指定group name，若不指定group name则属于默认的group

#### Leader
- 每个partition有多个副本，其中有且仅有一个作为Leader，Leader是当前负责数据的读写的partition

#### Follower
- Follower跟随Leader，所有写请求都通过Leader路由，数据变更会广播给所有Follower，Follower与Leader保持数据同步。
- 当Follower与Leader挂掉、卡住或者同步太慢，leader会把这个follower从“同步中副本”“in sync replicas”（ISR）列表中删除，重新创建一个Follower。

### 2.3.kafka安装
#### 3.1 windows环境单点安装
[kafka安装与命令行调用](https://www.cnblogs.com/riches/p/11715012.html)
```
1.Kafka下载地址  http://kafka.apache.org/downloads

2.解压文件 

3.修改配置：config下server.properties文件，把listeners的配置放开
listeners=PLAINTEXT://:9092

// 一些配置项说明
//#1、broker实例id 这个是kafka集群区分每个节点的唯一标志符
broker.id=0
//#2、监听的端口号
listeners=PLAINTEXT://:9092
//#3、消息的存放路径
log.dirs=E:\zhanghaoBF\【完成】SpringBoot整合kafka\kafka_2.11-2.3.0\zhanghaoBFkafkakafka_2.11-2.3.0kafka-logs
//#4、kafka消息的保留时间，默认为7天即168小时【单位为：小时】
log.retention.hours=168
//#5、log文件存放着msg，这里设置的是log文件的最大大小，默认最大为1个G【单位为：byte】
log.segment.bytes=1073741824
//#6、kafka链接的zk地址
zookeeper.connect=localhost:2181
//#7、kafka链接的超时时间
zookeeper.connection.timeout.ms=6000
//#8、默认的分区数量
num.partitions=1

4.启动：
//前提：已部署启动zk或者启动自带的zookeeper "./zookeeper-server-start.sh …/config/server.properties&"
进目录：cd D:\kafka_2.11-2.3.0\bin\windows
执行命令：kafka-server-start.bat ..\..\config\server.properties
///启动成功：
[2020-11-27 10:15:39,749] INFO Kafka version: 2.3.0 (org.apache.kafka.common.utils.AppInfoParser)
[2020-11-27 10:15:39,751] INFO Kafka commitId: fc1aaa116b661c8a (org.apache.kafka.common.utils.AppInfoParser)
[2020-11-27 10:15:39,751] INFO Kafka startTimeMs: 1606443339746 (org.apache.kafka.common.utils.AppInfoParser)
[2020-11-27 10:15:39,755] INFO [KafkaServer id=0] started (kafka.server.KafkaServer)

5.验证
进目录：cd D:\kafka_2.11-2.3.0\bin\windows

//创建topic
执行命令：kafka-topics.bat --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic testDemo
执行结果：Created topic testDemo.

//replication-factor：表示在不同的broker中保存几份副本。设置为1表示该topic除了当前leader主机外，在其他brocker上还有1个副本。
//partitions：表示分区数。设置为1表示该topic（同一个brocker上）存在有多少个分区。PS：每个分区的数据是不重复的，用于提高吞吐量。

//查看topic，##查看localhost:2181上存在的topic信息
执行命令：kafka-topics.bat --list --zookeeper localhost:2181
执行结果：testDemo

// #删除topic
bin/kafka-topics.sh --delete --zookeeper node1:2181 --topic test

//启动生产者
title producer
kafka-console-producer.bat --broker-list localhost:9092 --topic testDemo
>
//输入消息内容：
>hello kafka!!
>oho
>???
>test
>test2
>test3
>

//启动消费者
title consumer
kafka-console-consumer.bat --bootstrap-server localhost:9092 --topic testDemo --from-beginning
//展示消息内容
hello kafka!!
oho
???
test
test2
test3

```
PS：某些博文里会推荐使用‘kafka-console-consumer.bat --zookeeper localhost:2181 --topic’ testDemo  
这个命令来启动消费者，这里说明一下，低版本的kafka是用这个命令来启动消费者并进行消息消费的；  
由于本文使用的kafka版本较高，所以才用的上面那个命令行进行消费者启动，并消费信息。

[[工作中爬过的坑] Kafka配置域名的三种难度](https://blog.csdn.net/u013686990/article/details/103026842/)

#### 3.2 Linux环境单点[安装](https://blog.csdn.net/weixin_39393048/article/details/105058791)
```
1.Kafka下载地址  http://kafka.apache.org/downloads

2.上传、授权、解压文件
chmod 755 kafka_2.11-2.3.0.tgz
tar zxvf kafka_2.11-2.3.0.tgz

3.修改配置：config下server.properties文件，把listeners的配置放开
listeners=PLAINTEXT://:9092

// 一些配置项说明
//#1、broker实例id 这个是kafka集群区分每个节点的唯一标志符
broker.id=0
//#2、监听的端口号
listeners=PLAINTEXT://:9092
//#3、消息的存放路径
log.dirs=E:\zhanghaoBF\【完成】SpringBoot整合kafka\kafka_2.11-2.3.0\zhanghaoBFkafkakafka_2.11-2.3.0kafka-logs
//#4、kafka消息的保留时间，默认为7天即168小时【单位为：小时】
log.retention.hours=168
//#5、log文件存放着msg，这里设置的是log文件的最大大小，默认最大为1个G【单位为：byte】
log.segment.bytes=1073741824
//#6、kafka链接的zk地址
zookeeper.connect=localhost:2181
//#7、kafka链接的超时时间
zookeeper.connection.timeout.ms=6000
//#8、默认的分区数量
num.partitions=1

4.启动：
//前提：已部署启动zk或者启动自带的zookeeper "./zookeeper-server-start.sh …/config/server.properties&"
进目录：cd /opt/kafka_2.11-2.3.0/bin
执行命令：./kafka-server-start.sh ../config/server.properties &
///启动成功：
[2020-11-26 18:56:09,666] INFO Kafka version: 2.3.0 (org.apache.kafka.common.utils.AppInfoParser)
[2020-11-26 18:56:09,666] INFO Kafka commitId: fc1aaa116b661c8a (org.apache.kafka.common.utils.AppInfoParser)
[2020-11-26 18:56:09,666] INFO Kafka startTimeMs: 1606445769664 (org.apache.kafka.common.utils.AppInfoParser)
[2020-11-26 18:56:09,667] INFO [KafkaServer id=0] started (kafka.server.KafkaServer)


5.验证 
//创建topic
执行命令：./kafka-console-producer.sh --broker-list localhost:9092 --topic testDemo
执行结果：Created topic testDemo. 

//查看topic，##查看localhost:2181上存在的topic信息
执行命令：./kafka-topics.sh --list --zookeeper localhost:2181
执行结果：testDemo

// #删除topic
./kafka-topics.sh --delete --zookeeper node1:2181 --topic test

//启动生产者
./kafka-console-producer.sh --broker-list localhost:9092 --topic testDemo
>
//输入消息内容：
>hello kafka!!
>oho
>???
>test
>test2
>test3
>

//启动消费者
./kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic testDemo --from-beginning
//展示消息内容
hello kafka!!
oho
???
test
test2
test3
```

#### 3.3 Linux环境集群[安装](https://www.cnblogs.com/CQ-LQJ/p/11617126.html)
- https://blog.csdn.net/BoomLee/article/details/105617323#%C2%A01.%E4%B8%8B%E8%BD%BD  
- https://blog.csdn.net/Poppy_Evan/article/details/79415460?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-2.control&depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-2.control
- Linux搭建 zookeeper+kafka 集群：https://blog.csdn.net/yuanlong122716/article/details/104830491

### 2.4.Spring集成使用kafka 
- [SpringBoot整合kafka(实现producer和consumer)](https://www.jianshu.com/p/5da86afed228)   
- [SpringBoot集成kafka全面实战](https://blog.csdn.net/yuanlong122716/article/details/105160545/)  
```
//1.添加依赖
<dependency>
    <groupId>org.springframework.kafka</groupId>
    <artifactId>spring-kafka</artifactId>
</dependency>

//2.添加配置
spring:   
  kafka:
    bootstrap-servers: 127.0.0.1:9092 #指定kafka server的地址，集群配多个，中间，逗号隔开
    producer:
      # Kafka提供的序列化和反序列化类
      key-serializer: org.apache.kafka.common.serialization.StringSerializer
      value-serializer: org.apache.kafka.common.serialization.StringSerializer
    consumer:
      group-id: default_consumer_group #群组ID
      enable-auto-commit: true
      # 提交offset延时(接收到消息后多久提交offset)
      auto-commit-interval: 1000
      # Kafka提供的序列化和反序列化类
      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      value-deserializer: org.apache.kafka.common.serialization.StringDeserializer
    listener:
      # 消费端监听的topic不存在时，项目启动会报错(关掉)
      missing-topics-fatal: false

//3.简单生产者Producer
@RestController
public class ProducerController {
    @Resource
    private KafkaTemplate<String, Object> kafkaTemplate;

    @Autowired
    private KafkaTemplate<String, String> kafkaTemplate1;

    @RequestMapping("message/send")
    public String send(){
        //使用kafka模板发送信息，newTopic：topic不存在时会自行创建
        kafkaTemplate.send("newTopic", "This is msg");
        kafkaTemplate1.send("testDemo","This is msg too");
        return "success";
    }
}

//4.简单消费者Consumer 
@Component
public class Consumer {
    /**
     * 定义此消费者接收topics = "testDemo"的消息，与producerController中的topic对应上即可
     * @param record 变量代表消息本身，可以通过ConsumerRecord<?,?>类型的record变量来打印接收的消息的各种信息
     */
    @KafkaListener(topics = "newTopic")
    public void listen (ConsumerRecord<?, ?> record){
        System.out.printf("topic is %s, offset is %d, value is %s \n", record.topic(), record.offset(), record.value());
    }

    @KafkaListener(topics = "testDemo")
    public void listen2 (ConsumerRecord<?, ?> record){
        System.out.printf("topic is %s, offset is %d, value is %s \n", record.topic(), record.offset(), record.value());
    }
}

```

### 2.5 spring-cloud-stream使用kafka
官方定义 Spring Cloud Stream 是一个构建消息驱动微服务的框架  
- [SpringCloud实战9-Stream消息驱动](https://www.cnblogs.com/huangjuncong/p/9102843.html)
- [Spring Cloud 应用篇 之 Spring Cloud Stream（消息驱动）](https://blog.csdn.net/hubo_88/article/details/80904165)

``` 

```

## 三、RocketMQ




## 四、RabbitMQ


## 五、问题整理
### 5.1 消息可靠性

### 5.2 消息丢失

### 5.3 消息重复消费



